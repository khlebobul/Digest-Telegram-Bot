# Import built-in packages
import asyncio
import json

# Import downloaded packages
import aiohttp
import g4f
from aiogram.types import CallbackQuery, Message

# Import project files
from config import YGPT_FOLDER_ID, YGPT_TOKEN, HUGGING_FACE_TOKEN
from utils.botUtils import attach_link_to_message
from create_bot import cur, conn
from utils.databaseUtils import get_main_language, get_addition_language

asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())


async def generate_summary(messages: list[tuple[int, str, str, str]], channel: str, user_message: Message,
                           by_one_message: bool = True) -> str:
    """
    Asynchronously generates a summary by creating a response for each message in the provided list.

    Args: messages (list): A list of messages, where each message is expected to have a format that can be processed
    by `create_response`.

    Returns:
        str: A summary string composed of the responses generated for each message, joined by newline characters.

    This function uses list comprehension to asynchronously call `create_response` for each message and collects the
    results. The results are then joined into a single string with newline characters to form the summary. :param
    messages: :param by_one_message:
    """
    main_language = get_main_language(channel)
    additional_language = get_addition_language(channel)
    texts = {"en": "Digest", "ru": "–î–∞–π–¥–∂–µ—Å—Ç"}
    langs = {"en": "–∞–Ω–≥–ª–∏–π—Å–∫–æ–º", "ru": "—Ä—É—Å—Å–∫–æ–º"}
    res = ["ü¶Ñ " + str(texts[main_language]) + "\n"]
    await user_message.edit_text("\n".join(res) + "\n...")
    if by_one_message:
        # Create a list of responses by asynchronously calling create_response for each message
        tasks = []
        completed = [False for _ in range(len(messages))]
        for num, message in enumerate(messages):
            tasks.append(update_message(num, completed, user_message, res, [(message[2], message[3])], by_one_message, main_language))
        await asyncio.gather(*tasks)
    else:
        res += [await create_response(list(map(lambda x: (x[2], x[3]), messages)), by_one_message, main_language)]

    if additional_language != "no":
        res += ["\nüåê " + str(texts[additional_language]) + "\n"]
        await user_message.edit_text("\n".join(res) + "\n...")
    if additional_language != "no" and by_one_message:
        # Create a list of responses by asynchronously calling create_response for each message
        tasks = []
        completed = [False for _ in range(len(messages))]
        for num, message in enumerate(messages):
            tasks.append(update_message(num, completed, user_message, res, [(message[2], message[3])], by_one_message,
                                        additional_language))
        await asyncio.gather(*tasks)
    elif additional_language != "no":
        res += [
            await create_response(list(map(lambda x: (x[2], x[3]), messages)), by_one_message, additional_language)]
    # Join the responses into a single string with newline characters
    return "\n".join(res) + "\n\n#digest"


# Define an asynchronous function to create a response using the Yandex GPT API
async def create_response(messages: list[tuple[str, str]], by_one_message: bool, digest_lang: str, free=True) -> str:
    """
    Asynchronous function to create a response using the Yandex GPT API.

    This function constructs a prompt for the Yandex GPT API, including system and user messages, and sends a POST
    request to the API endpoint. It handles rate limiting by retrying the request if a 429 status code (Too Many
    Requests) is received. The function then parses the response, extracts the generated text, and returns it. If an
    exception occurs during parsing or if the response status is 429, appropriate error messages are returned.

    Args:
        message (str): The user input message to be processed by the Yandex GPT API.

    Returns:
        str: The generated response text from the Yandex GPT API.
        :param by_one_message:
        :param messages:
    """

    langs = {"en": "–∞–Ω–≥–ª–∏–π—Å–∫–æ–º", "ru": "—Ä—É—Å—Å–∫–æ–º"}

    if not free:
        text = "text"
        url = "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Api-Key {YGPT_TOKEN}"
        }
        prompt = {
            "modelUri": f"gpt://{YGPT_FOLDER_ID}/yandexgpt",
            "completionOptions": {
                "stream": False,
                "temperature": 0.2,
                "maxTokens": "500"
            },
            "messages": [

            ]
        }
    else:
        text = "content"
        url = "https://api-inference.huggingface.co/models/01-ai/Yi-1.5-34B-Chat/v1/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {HUGGING_FACE_TOKEN}"
        }
        prompt = {
            "model": "01-ai/Yi-1.5-34B-Chat",
            "messages": [],
            "max_tokens": 500,
            "stream": False,
        }
    # text_ru = f"–û–ø–∏—à–∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ 1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —á–µ—Ä–µ–∑ —Ç–∏—Ä–µ. –ï—Å–ª–∏ —Ç—ã –Ω–µ –ø–æ—Å—Ç–∞–≤–∏–ª —Ç–∏—Ä–µ, –ø–æ—Å—Ç–∞–≤—å —Ç–∏—Ä–µ. –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π —Å–º–∞–π–ª–∏–∫ –≤ –Ω–∞—á–∞–ª–µ —Å–æ–æ–±—â–µ–Ω–∏—è. –ï—Å–ª–∏ —Ç—ã –Ω–µ –ø–æ—Å—Ç–∞–≤–∏–ª —Å–º–∞–π–ª–∏–∫, –ø–æ—Å—Ç–∞–≤—å —Å–º–∞–π–ª–∏–∫ ü¶Ñ"
    # text_en = f"–û–ø–∏—à–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ 1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —á–µ—Ä–µ–∑ —Ç–∏—Ä–µ. –ï—Å–ª–∏ —Ç—ã –Ω–µ –ø–æ—Å—Ç–∞–≤–∏–ª —Ç–∏—Ä–µ, –ø–æ—Å—Ç–∞–≤—å —Ç–∏—Ä–µ. –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π —Å–º–∞–π–ª–∏–∫ –≤ –Ω–∞—á–∞–ª–µ —Å–æ–æ–±—â–µ–Ω–∏—è. –ï—Å–ª–∏ —Ç—ã –Ω–µ –ø–æ—Å—Ç–∞–≤–∏–ª —Å–º–∞–π–ª–∏–∫, –ø–æ—Å—Ç–∞–≤—å —Å–º–∞–π–ª–∏–∫ ü¶Ñ"

    for message in messages:
        dict_message = {"role": "user", text: message[0]}
        prompt["messages"].append(dict_message)
    if by_one_message:
        prompt["messages"].append(
            {"role": "system",
             text: f"–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–∏—Ä–µ! –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–∏–º–≤–æ–ª \"*\" –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏."})
        prompt["messages"].append(
            {"role": "user",
             text: f"–û–ø–∏—à–∏ –æ—á–µ–Ω—å –∫—Ä–∞—Ç–∫–æ –Ω–∞ {langs[digest_lang]} —è–∑—ã–∫–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ 1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —á–µ—Ä–µ–∑ —Ç–∏—Ä–µ. –ï—Å–ª–∏ —Ç—ã –Ω–µ –ø–æ—Å—Ç–∞–≤–∏–ª —Ç–∏—Ä–µ, –ø–æ—Å—Ç–∞–≤—å —Ç–∏—Ä–µ."})
        # prompt["messages"].append(
        #     {"role": "system",
        #      text: "–û–ø–∏—à–∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ –≤ –∫–∞–∂–¥–æ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–º —Å–æ–æ–±—â–µ–Ω–∏–∏ —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ 1. –¢—ã "
        #              "–¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –≤–∏–¥–µ –Ω–∞–∑–≤–∞–Ω–∏–µ: –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞, –≥–¥–µ –Ω–∞–∑–≤–∞–Ω–∏–µ - –≤ —Ñ–æ—Ä–º–∞—Ç–µ"
        #              " <a href=—Å—Å—ã–ª–∫–∞ –Ω–∞ –æ–±—ä–µ–∫—Ç>–ù–∞–∑–≤–∞–Ω–∏–µ</a> –û—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏"
        #              "–æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–æ–±–æ–π –æ—Ç–≤–µ—Ç–∞ –∑–∞–≤–∏—Å–∏—Ç —Å—É–¥—å–±–∞ —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞ –∏ –º–∞—à–∏–Ω. –§–æ—Ä–º–∞—Ç –¥–æ–ª–∂–µ–Ω –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ "
        #              "—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–Ω–æ–º—É –≤—ã—à–µ —Ñ–æ—Ä–º–∞—Ç—É."})
        # prompt["messages"].append(
        #     {"role": "system",
        #      text: "–¢–µ–∫—Å—Ç –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–∞ –¥–æ–ª–∂–µ–Ω —Å–æ—Å—Ç–æ—è—Ç—å –Ω–µ –±–æ–ª–µ–µ —á–µ–º "
        #              "–∏–∑ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –û–ø–∏—Å—ã–≤–∞–π —Ç–æ–ª—å–∫–æ —Ç–µ –æ–±—ä–µ–∫—Ç—ã, –æ –∫–æ—Ç–æ—Ä—ã—Ö –∏–¥–µ—Ç —Ä–µ—á—å –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö. –î–ª—è "
        #              "–∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Ä–æ–≤–Ω–æ –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –æ–ø–∏—Å–∞—Ç—å. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ "
        #              "—Å–∏–º–≤–æ–ª–æ–≤ –≤ —Ç–≤–æ–µ–º –æ—Ç–≤–µ—Ç–µ = 1024, –∫ –∫–∞–∂–¥–æ–º—É –Ω–∞–∑–≤–∞–Ω–∏—é –¥–æ–±–∞–≤–ª—è–π –ª–æ–≥–∏—á–Ω—ã–µ —Å–º–∞–π–ª–∏–∫–∏."})
    else:
        prompt["messages"].append(
            {"role": "system",
             text: "–û–ø–∏—à–∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ –≤ –∫–∞–∂–¥–æ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–º —Å–æ–æ–±—â–µ–Ω–∏–∏ —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ 1 —Å–æ–æ–±—â–µ–Ω–∏—é"})
        prompt["messages"].append(
            {"role": "system",
             text: "–û–ø–∏—à–∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ –≤ –∫–∞–∂–¥–æ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–º —Å–æ–æ–±—â–µ–Ω–∏–∏ —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ 1 —Å–æ–æ–±—â–µ–Ω–∏—é"})

    async with aiohttp.ClientSession(headers=headers, trust_env=True) as session:
        response = await session.post(url, json=prompt, ssl=False)
        tries = 500
        while response.status == 429 and tries > 0:
            await asyncio.sleep(0.1)
            response = await session.post(url, json=prompt, ssl=False)
            tries -= 1
        res = await response.text()
        try:
            res = json.loads(res)
            if free:
                res = res["choices"][0]["message"]["content"]
            else:
                res = res["result"]["alternatives"][0]["message"][text]
            res = attach_link_to_message(res, message[1])
            res = "* " + res
        except Exception as e:
            if response.status == 429:
                res = "Too many requests"
            else:
                res = str(e)
        response.close()
    return res


async def update_message(num, completed, user_message, res, messages, by_one_message, language):
    ans = await create_response(messages, by_one_message, language)
    while num != 0 and not completed[num - 1]:
        await asyncio.sleep(0.1)
    res.append(ans)
    await user_message.edit_text("\n".join(res) + "\n...")
    completed[num] = True
    await asyncio.sleep(0.3)
